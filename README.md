# AI-XAI-LLM

## Requirements

- Python 3.8+
- Virtual environment (venv)
- LM Studio (for LLM-based explanations)
- Internet connection (for Kaggle dataset download)

## Setup Instructions

### 1. Clone or Download the Project

```bash
cd /path/to/stroke_prediction
```

### 2. Create Virtual Environment

**On Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**On Windows (PowerShell):**
```powershell
python -m venv venv
.\venv\Scripts\Activate.ps1
```

**On Windows (Git Bash):**
```bash
python -m venv venv
source venv/Scripts/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Verify Installation

**Check Python version:**
```bash
python --version
# Should show Python 3.8 or higher
```

**Verify packages installed:**
```bash
pip list
```

**Test imports:**
```bash
python -c "import sklearn, pandas, numpy, shap, imblearn; print('All imports successful!')"
``` (>=0.2.0)

### 4. Verify Installation

**Check Python version:**
```bash
python --version
# Should show Python 3.8 or higher
```

**Verify packages installed:**
```bash
pip list
```

**Test imports:**
```bash
python -c "import sklearn, pandas, numpy, shap, imblearn; print('All imports successful!')"
```

### 5. Set Up LM Studio (for Instance Explanation)

1. Download and install [LM Studio](https://lmstudio.ai/)
2. Download a compatible model (e.g., `google/gemma-3n-e4b`)
3. Start the local server:
   - Open LM Studio
   - Load the model
   - Click "Start Server" (default: http://localhost:1234)

### 6. Make Shell Scripts Executable (Linux/Mac/Git Bash)

```bash
chmod +x datapreprocessor.sh
chmod +x modeltrainer.sh
chmod +x instanceexplainer.sh
```

## Usage

### Step 1: Data Preprocessing

Run the data preprocessing script to download the dataset, perform EDA, preprocess data, create representative sample, and apply SMOTE:

**On Linux/Mac/Git Bash:**
```bash
./datapreprocessor.sh
```

**On Windows (PowerShell):**
```powershell
.\datapreprocessor.sh
# OR if you have Git Bash installed
bash datapreprocessor.sh
# OR run Python directly
python datapreprocessor.py
```

**Output files:**
- `data/representative_sample.csv` - 15 diverse patient samples (5 stroke, 10 non-stroke)
- `data/X_smote.csv`, `data/y_smote.csv` - SMOTE-balanced training data
- `data/zscore_scaler.pkl` - StandardScaler for normalization
- `data/ordinal_encoder.pkl` - Encoder for categorical features
- `data/knn_imputer.pkl` - KNN imputer for missing values
- `data/feature_names.pkl` - List of feature names
- `plots/*.png` - EDA visualization plots

**Expected runtime:** 2-5 minutes (depending on internet speed for dataset download)

---

### Step 2: Model Training

Run the model training script to perform hyperparameter tuning, cross-validation, and train the final model:

**On Linux/Mac/Git Bash:**
```bash
./modeltrainer.sh
```

**On Windows (PowerShell):**
```powershell
.\modeltrainer.sh
# OR if you have Git Bash installed
bash modeltrainer.sh
# OR run Python directly
python modeltrainer.py
```

**Output files:**
- `models/rf_stroke_model.pkl` - Trained Random Forest model
- `models/rf_features.pkl` - Feature names used by the model
- `models/best_params.pkl` - Optimal hyperparameters from GridSearchCV
- `models/cv_results.csv` - Detailed cross-validation results for each fold
- `models/cv_summary.csv` - Summary statistics (mean Â± std) for all metrics
- `plots/feature_importance.png` - Feature importance plot

**Expected runtime:** 10-30 minutes (depending on system performance)

**Cross-validation metrics tracked:**
- Accuracy
- Precision
- Recall (primary optimisation metric)
- F1 Score
- ROC-AUC

---

### Step 3: Instance Explanation

Run the instance explanation script to generate SHAP-based explanations and LLM-powered clinical reports for a specific patient:

**On Linux/Mac/Git Bash:**
```bash
./instanceexplainer.sh <patient_index>
```

**On Windows (PowerShell):**
```powershell
.\instanceexplainer.sh <patient_index>
# OR if you have Git Bash installed
bash instanceexplainer.sh <patient_index>
# OR run Python directly
python instanceexplainer.py <patient_index>
```

**Arguments:**
- `<patient_index>`: Integer between 0 and 14 (patient index in representative sample)

**Example:**
```bash
./instanceexplainer.sh 12
```

**Output files:**
- `reports/patient_<index>_report.txt` - Clinical stroke risk report with SHAP explanations and LLM interpretation

**Expected runtime:** 30 seconds - 2 minutes (depending on LLM response time)

**Report contents:**
- Patient demographic and clinical data
- Stroke risk prediction (High/Low) with probability
- Top 3 SHAP-contributing features
- Human-readable clinical explanation generated by LLM

**Generate reports for all patients:**

**Linux/Mac/Git Bash:**
```bash
# Generate reports for all 15 patients
for i in {0..14}; do ./instanceexplainer.sh $i; done
```

**Windows PowerShell:**
```powershell
# Generate reports for all 15 patients
0..14 | ForEach-Object { python instanceexplainer.py $_ }
```

---

## Example Workflow

```bash
# 1. Activate virtual environment
source venv/bin/activate  # Linux/Mac
# OR
.\venv\Scripts\Activate.ps1  # Windows PowerShell

# 2. Preprocess data
./datapreprocessor.sh

# 3. Train model
./modeltrainer.sh

# 4. Generate explanations for different patients
./instanceexplainer.sh 0   # First stroke patient
./instanceexplainer.sh 5   # First non-stroke patient
./instanceexplainer.sh 12  # Another patient
```

## Alternative: Legacy All-in-One Scripts

For quick testing or educational purposes, you can use the legacy scripts that combine multiple steps:

### trained_ml.py - Complete ML Pipeline

Runs the entire ML pipeline (EDA, preprocessing, training, evaluation) in one script:

```bash
python trained_ml.py
```

**What it does:**
- Downloads and loads the dataset
- Performs exploratory data analysis (EDA)
- Preprocesses data (encoding, imputation, normalization)
- Selects representative samples
- Applies SMOTE for class balancing
- Performs hyperparameter tuning with GridSearchCV
- Trains and evaluates the model with 5-fold cross-validation
- Saves all artifacts to the root directory

**Runtime:** ~15-25 minutes

**Output:** Saves model artifacts (`rf_stroke_model.pkl`, `rf_features.pkl`, etc.) and `representative_sample.csv` to the root directory.

### prediction_sys.py - Prediction & Explanation System

Generates predictions and explanations for a predefined patient:

```bash
# Ensure LM Studio is running first!
python prediction_sys.py
```

**What it does:**
- Loads trained model and artifacts
- Loads representative sample
- Makes predictions for a selected patient (configurable in script)
- Generates SHAP explanations
- Uses LLM to create clinical explanations
- Displays results in console

**Prerequisites:** 
- Must run `trained_ml.py` first to generate model artifacts
- LM Studio must be running

**Runtime:** ~30 seconds - 1 minute

**Note:** The legacy scripts save artifacts to the root directory, while the modular workflow uses subdirectories (`data/`, `models/`, `plots/`, `reports/`).

## Important Notes

**Due to cross-validation and the use of the language model, these scripts will produce slightly different results each time they are executed.**
